{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Lines\n",
    "#### Detection of Lane Lines using Advanced CV Techniques\n",
    "In this project we will be using advanced techniques for finding lane lines compared with the first project.  Techniques used will include edge detection, image thresholding, perspective transforms and fitting polynomials for finding and detecting lane lines in camera images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpful plotting function\n",
    "def plot(images, no_rows, no_cols, width = 10., height = 4., color='gray'):\n",
    "    fig = plt.figure(1, (width, height))\n",
    "    i = 0\n",
    "    \n",
    "    for image in images:\n",
    "        sub = fig.add_subplot(no_rows, no_cols, i + 1)\n",
    "        sub.imshow(image, color)\n",
    "        sub.axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Calibration\n",
    "Prior to detecting lane lines we need to first calibrate the camera to account for lense distortion common in all modern day cameras.  Once we have a calibrated camera image, we can then accurately use perspective transformation for computing line curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set chessboard corner sizes\n",
    "nb_cornerx = 9\n",
    "nb_cornery = 6\n",
    "# Set calibration image path\n",
    "calibration_path = \"./camera_cal/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions used to calibrate the camera\n",
    "Utilising the findChessboardCorners in OpenCV we can compute distortion coefficients to calibrate the camera and undistort any images affected by camera lense distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a Camera class with methods for calibration\n",
    "# and undistorting images\n",
    "class Camera():\n",
    "    \n",
    "    Matrix = None\n",
    "    Distortion = None\n",
    "    ImageSize = None\n",
    "    \n",
    "    def __init__(self, mtx = None, dist = None):\n",
    "        self.Matrix = mtx\n",
    "        self.Distortion = dist\n",
    "    \n",
    "    # Define corner detection\n",
    "    def find_corners(self, img, dimensions):\n",
    "        # Define translation and distortion parameters\n",
    "        objpoints = []\n",
    "        imgpoints = []\n",
    "\n",
    "        # Initialise chessboard point space parameter\n",
    "        objp = np.zeros((dimensions[1] * dimensions[0], 3), np.float32)\n",
    "        objp[:,:2] = np.mgrid[0:dimensions[0], 0:dimensions[1]].T.reshape(-1, 2) # x,y coordinates\n",
    "\n",
    "        # convert to grayscale for detection\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        # find corners usng grayscale image\n",
    "        [ret, corners] = cv2.findChessboardCorners(gray, dimensions, None)\n",
    "\n",
    "        return [ret, objp, corners]\n",
    "    \n",
    "    # Camera calibration function\n",
    "    def calibrate(self, images, dimensions):\n",
    "        # Define real-world and image space coordinate arrays\n",
    "        objpoints = []\n",
    "        imgpoints = []\n",
    "\n",
    "        self.ImageSize = (images[0].shape[1], images[0].shape[0])\n",
    "\n",
    "        for img in images:\n",
    "            # detect corners in the chessboard calibration image\n",
    "            [ret, points, corners] = self.find_corners(img, dimensions)\n",
    "\n",
    "            if (ret == True):\n",
    "                # add the real-world points along with image space points to the arrays\n",
    "                objpoints.append(points)\n",
    "                imgpoints.append(corners)\n",
    "        \n",
    "        # calibrate camera using point spaces\n",
    "        [ret, mtx, dist, rvecs, tvecs] = cv2.calibrateCamera(objpoints, imgpoints, self.ImageSize, None, None)    \n",
    "        \n",
    "        # store calibration\n",
    "        self.Matrix = mtx\n",
    "        self.Distortion = dist\n",
    "        \n",
    "        return [self.Matrix, self.Distortion]\n",
    "    \n",
    "    ### Define the warping functions ###\n",
    "    def transform(self, src, dst):\n",
    "        if (src is not None and dst is not None):\n",
    "            self.M = cv2.getPerspectiveTransform(src, dst)\n",
    "        if (src is not None and dst is not None):\n",
    "            self.Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    # Warp an image using the specified source and destination coordinate mappings.\n",
    "    def warp(self, img):\n",
    "        return cv2.warpPerspective(img, self.M, self.ImageSize, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Unwarps an image using the specified source and destination coordinate mappings.\n",
    "    def unwarp(self, img):\n",
    "        return cv2.warpPerspective(img, self.Minv, self.ImageSize, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Image undistortion using translation coefficients\n",
    "    def undistort(self, img):\n",
    "        return cv2.undistort(img, self.Matrix, self.Distortion, None, self.Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a camera reference for the scope of the project*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Camera Reference ###\n",
    "camera = Camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the calibration images with detected chessboard lines__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# fill calibration images\n",
    "calibration_images = [mpimg.imread(os.path.join(calibration_path, x)) for x in os.listdir(calibration_path)]\n",
    "\n",
    "chessboard_images = []\n",
    "for img in calibration_images:\n",
    "    [ret, objpoints, corners] = camera.find_corners(img, (nb_cornerx, nb_cornery))\n",
    "    # if corners found, draw them on the image\n",
    "    if (ret == True):\n",
    "        img = cv2.drawChessboardCorners(img, (nb_cornerx, nb_cornery), corners, ret)\n",
    "        chessboard_images.append(img)\n",
    "\n",
    "# plot the images after corner detection and drawing lines\n",
    "plot(chessboard_images[3:7], 1, 4, 12., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show a single calibration image, before and after distortion correction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Camera Calibration ###\n",
    "# calibrate the camera using the calibration chessboard images and dimensions\n",
    "[camera_mtx, camera_dist] = camera.calibrate(calibration_images, (nb_cornerx, nb_cornery))\n",
    "# show the result\n",
    "plot([calibration_images[0], camera.undistort(calibration_images[0])], 1, 2, 12., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show a road scene image before and after applying distortion correction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define sample road scene test images\n",
    "test1 = \"./test_images/test4.jpg\"\n",
    "test2 = \"./test_images/test5.jpg\"\n",
    "# undistort each\n",
    "undistorted_examples = [mpimg.imread(test1), camera.undistort(mpimg.imread(test1)), \\\n",
    "                        mpimg.imread(test2), camera.undistort(mpimg.imread(test2))]\n",
    "# plot each side by side with undistorted versions\n",
    "plot(undistorted_examples, 2, 2, 12., 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As per the images above, you can see that the right column of images, after distortion correction, appear more clipped on both sides of the images.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane Detection\n",
    "*Detecting lane line regions using colour maps, thresholds and gradients*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the pipeline used for producing a binary image as a result of applying colour thresholding, gradients and other techniques for extracting possible lane line pixels** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Lane Line helper functions ###\n",
    "\n",
    "# Masks using colour thresholding\n",
    "# - thresholds: 2D array of threshold values for each colour channel\n",
    "def mask_thresholds(img, thresholds = [(220,255), (0,255), (0,90)], mask = 1):\n",
    "    # init empty binary image map\n",
    "    binary_result = np.zeros((img.shape[0], img.shape[1]))\n",
    "    # apply thresholds for each channel\n",
    "    for t in range(len(thresholds)):\n",
    "        channel = img[:,:,t]\n",
    "        binary_result[((binary_result == mask) \\\n",
    "                       | ((channel >= thresholds[t][0]) & (channel <= thresholds[t][1])))] = mask\n",
    "    \n",
    "    return binary_result\n",
    "\n",
    "# Computes the raw sobel values for each direction (x and y) for a given image\n",
    "# - img: Input image in RGB format\n",
    "# - kernel: Scalar value >= 3, for the kernel size\n",
    "def compute_sobel(img, kernel):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # compute sobel derivatives\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=kernel)\n",
    "    \n",
    "    return [sobelx, sobely]\n",
    "\n",
    "# Computes the sobel threshold binary map for a given image\n",
    "# - img: Input image in RGB format\n",
    "# - kernel: Scalar value >= 3, for the kernel size\n",
    "# - thresh_x: 2D Tuple of threshold values for the x coordinate\n",
    "# - thresh_y: 2D Tuple of threshold values for the y coordinate\n",
    "def sobel_threshold(img, kernel = 3, thresh_x = (20, 100), thresh_y = (0, 255), mask = 1):\n",
    "    # compute sobel\n",
    "    [sobelx, sobely] = compute_sobel(img, kernel)\n",
    "    \n",
    "    # take the max values\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # scale to 0-255 range\n",
    "    scaled_sobelx = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    scaled_sobely = np.uint8(255*abs_sobely/np.max(abs_sobely))\n",
    "    \n",
    "    # apply threshold values\n",
    "    binary_output = np.zeros_like(abs_sobelx)\n",
    "    if (thresh_x is not None):\n",
    "        binary_output[((scaled_sobelx >= thresh_x[0]) & (scaled_sobelx <= thresh_x[1]))] = mask\n",
    "    \n",
    "    if (thresh_y is not None and thresh_x is not None):\n",
    "        binary_output[(((scaled_sobely >= thresh_y[0]) & (scaled_sobely <= thresh_y[1]))\\\n",
    "                      & (binary_output == mask))] = mask\n",
    "    elif (thresh_y is not None):\n",
    "        binary_output[((scaled_sobely >= thresh_y[0]) & (scaled_sobely <= thresh_y[1]))] = mask\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "# Computes the sobel magnitude binary map for a given image\n",
    "# - img: Input image in RGB format\n",
    "# - kernel: Scalar value >= 3, for the kernel size\n",
    "# - thresh: 2D Tuple of threshold values\n",
    "def sobel_magnitude(img, kernel = 3., thresh = (20, 100), mask = 1):\n",
    "    # compute sobel\n",
    "    [sobelx, sobely] = compute_sobel(img, kernel)\n",
    "    \n",
    "    # take the max values\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # compute the magnitude\n",
    "    mag_sobel = np.sqrt(abs_sobelx**2. + abs_sobely**2.)\n",
    "    # scale within 0-255 range\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    # apply threshold values\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = mask\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "# Computes the sobel direction gradient for a given image\n",
    "# - img: Input image in RGB format\n",
    "# - kernel: Scalar value >= 3, for the kernel size\n",
    "# - thresh: 2D Tuple of threshold values\n",
    "def sobel_direction(img, kernel=3, thresh = (0, np.pi/2), mask = 1):\n",
    "    # compute sobel\n",
    "    [sobelx, sobely] = compute_sobel(img, kernel)\n",
    "    # take the max values\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # compute directional gradients\n",
    "    grad = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # create mask\n",
    "    binary_output = np.zeros_like(grad)\n",
    "    binary_output[(grad >= thresh[0]) & (grad <= thresh[1])] = mask\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all the helper CV functions into one function which returns a binary mask of the possible lane lines found within an image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lane Line Detection Pipeline ###\n",
    "\n",
    "# Applies a combination of CV techniques to an input image for masking lane lines \n",
    "# in an image.\n",
    "# - img: Input image in RGB format\n",
    "# - kernels: 3D tuple of kernel values for computing absolute, magnitude and directional sobel coordinates\n",
    "# - abs_thresh: 2D tuple of min and max threshold values for the absolute Sobel value mask\n",
    "# - grad_thresh: 2D tuple of min and max threshold values for the magnitude Sobel value mask\n",
    "# - dir_thresh: 2D tuple of min and max threshold values for the directional Sobel value mask\n",
    "# Returns: Binary mask where the mask value is a likely candidate lane line.\n",
    "def lane_line_mask(img, kernels, abs_thresh, mag_thresh, dir_thresh, mask = 1):\n",
    "    # compute sobel threshold, magnitude and directional gradients\n",
    "    sobel_abs = sobel_threshold(img, kernel=kernels[0], thresh_x=abs_thresh[0], thresh_y=abs_thresh[1], mask=mask)\n",
    "    sobel_mag = sobel_magnitude(img, kernel=kernels[1], thresh=mag_thresh, mask=mask)\n",
    "    sobel_dir = sobel_direction(img, kernel=kernels[2], thresh=dir_thresh, mask=mask)\n",
    "    # combine the sobel masks\n",
    "    combined = np.zeros_like(sobel_dir)\n",
    "    combined[((sobel_abs == mask) | ((sobel_mag == mask) & (sobel_dir == mask)))] = mask\n",
    "    # convert to YUV colour space\n",
    "    yuv_img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    # threshold the Y,U,V colour channels (ignore U channel)\n",
    "    yuv_mask = mask_thresholds(yuv_img, [(220,255), (0,0), (0,90)], mask)\n",
    "    # combine the detected lines from sobel and colour thresholding\n",
    "    result = np.zeros_like(combined)\n",
    "    result[((combined == mask) | (yuv_mask == mask))] = mask\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define test image path\n",
    "test_path = \"./test_images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show example masked images as a result of applying combined CV techniques like thresholding and masking for the purposes of finding lane lines in the input image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show example binary masking of test images\n",
    "\n",
    "# Init masking parameters\n",
    "mask = 255\n",
    "\n",
    "sobel_abs_thresh = [(20, 100), None] # absolute [x,y]\n",
    "sobel_mag_thresh = (30, 100) # 8, 110 # magnitude\n",
    "sobel_dir_thresh = (0.7, np.pi/2.) # direction\n",
    "\n",
    "sobel_kernels = (3, 9, 9) # absolute / magnitude / direction\n",
    "# Init test examples\n",
    "line_extraction_examples = [mpimg.imread(os.path.join(test_path, x)) for x in os.listdir(test_path)]\n",
    "# Images array of computed masks\n",
    "line_extracted_examples = []\n",
    "\n",
    "for example in line_extraction_examples:\n",
    "    mask_img = lane_line_mask(example, sobel_kernels, sobel_abs_thresh, sobel_mag_thresh, sobel_dir_thresh, mask)\n",
    "    line_extracted_examples.append(mask_img)\n",
    "\n",
    "plot(line_extracted_examples, 2, 4, 12., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see that in most cases it is quite evident where the lane line pixels lie.  We also have a number of features such as neighbouring lane lines and road edges, which will be useful later on...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane Line Finding Pipeline\n",
    "*Defines a pipeline for computing the lane lines, as detected Line objects, in each frame.  This is done by first computing the mask, then applying a perspective transform and using a sliding window to find the most likely candidate pixels of a lane.  The resulting pixel candidates are then processed using a polynomial function with the result drawn over the input image and the perspective returned to the appropriate view.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare perspective transforms\n",
    "img_size = camera.ImageSize\n",
    "p_offset = 30\n",
    "y_center = 440\n",
    "y_margin = 50\n",
    "x_margin = 320\n",
    "\n",
    "# Define our warp source and destination coordinates\n",
    "src_map = np.float32([[(img_size[0]/2.)+p_offset, y_center],  # top-right\n",
    "                      [img_size[0]-(x_margin-p_offset)*1, img_size[1]-y_margin*2.],  # bottom-right\n",
    "                      [(x_margin-p_offset)*1, img_size[1]-y_margin*2.],  # bottom-left\n",
    "                      [(img_size[0]/2.)-p_offset, y_center]]) # top-left \n",
    "\n",
    "dst_map = np.float32([[img_size[0]-(x_margin+p_offset), 0],    # top-right\n",
    "                      [img_size[0]-x_margin, img_size[1]],  # bottom-right\n",
    "                      [x_margin, img_size[1]],  # bottom-left\n",
    "                      [x_margin+p_offset, 0]])   # top-left\n",
    "\n",
    "# initialise camera perspective transforms\n",
    "camera.transform(src_map, dst_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the coordinates on an example\n",
    "test_img = line_extraction_examples[0]\n",
    "point_img = np.copy(test_img)\n",
    "\n",
    "randcolor = lambda x, y: (np.random.randint(x, y), np.random.randint(x, y), np.random.randint(x, y))\n",
    "\n",
    "for p1, p2 in zip(src_map, dst_map):\n",
    "    cv2.line(point_img, (p1[0], p1[1]), (p2[0], p2[1]), (200,200,200), 2)\n",
    "    plt.plot(p1[0], p1[1], \".\")\n",
    "    plt.plot(p2[0], p2[1], \".\")\n",
    "    \n",
    "plt.imshow(point_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective Transform\n",
    "*Apply perspective transformation to each image for the purpose of applying signal processing to detect the lane lines.*\n",
    "\n",
    "__Perspective warped examples using the camera warp function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warped_examples = []\n",
    "for example in line_extracted_examples:\n",
    "    warped = camera.warp(example)\n",
    "    warped_examples.append(warped)\n",
    "plot(warped_examples, 2, 4, 12., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As per the images above, there appears to be another lane signal appearing. This additional signal is the road edge which provides further cues as to the positions of the lane lines as well as lane curvature.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal Processing the Lane Lines\n",
    "Use signal processing to find the best candidate lane line pixels using a sliding window function over the likely candidate pixels.\n",
    "\n",
    "**Define the lane signal processing pipeline here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lane fitting function\n",
    "\n",
    "# Normalises the input to the value of 0-1.\n",
    "# - Returns: array of size X\n",
    "def normalise(X):\n",
    "    return X / np.sum(X, axis=0)\n",
    "\n",
    "# Finds the nearest element to the specified value\n",
    "# - Returns: index of nearest element\n",
    "def find_nearest(array, value):\n",
    "    return np.argmin(np.abs(array - value))\n",
    "\n",
    "# Returns the max value of a given signal\n",
    "# - Returns: Tuple of max index and corresponding value\n",
    "def compute_signal(signal, min_range, max_range, offset, threshold):\n",
    "    # normalise ranges\n",
    "    min_range_i = max(min(min_range, max_range), 0)\n",
    "    max_range_i = min(max(max_range, min_range), len(signal))\n",
    "    \n",
    "    if (max_range_i - min_range_i <= 0):\n",
    "        max_range_i = min(max_range_i + offset, len(signal))\n",
    "    \n",
    "    centerv = np.argmax(signal[min_range_i : max_range_i]) + min_range_i - offset\n",
    "    maxv = signal[centerv + offset]\n",
    "    \n",
    "    if (maxv <= threshold):\n",
    "        centerv = np.median([min_range_i, max_range_i]) - offset\n",
    "        maxv = signal[int(centerv)]\n",
    "        \n",
    "    return [centerv, maxv]\n",
    "\n",
    "# Fits valid lane pixel windows which correlate to prospective lane lines in an image.\n",
    "# - image: Input image (single channel)\n",
    "# - window_width: Width of the sliding window\n",
    "# - window_height: Height of the sliding window\n",
    "# - margin: Sliding window margin\n",
    "# - threshold: Value threshold for max convolution\n",
    "# - zoom: Zoom height margin for computing the greater window signal\n",
    "# - gain: memory weight for lane heading\n",
    "# - scan: memory length for ranging lane distance values\n",
    "# - memory: number of timesteps for computing lane heading\n",
    "# - theta: degrees of freedom of the lane heading\n",
    "# - gamma: thresholding value to keep discount factors within limits\n",
    "# - eps: epsilon parameter to avoid divide by zero errors\n",
    "def find_window_centroids(img, window_width, window_height, margin, threshold = 0.2, zoom = 2, \\\n",
    "                          gain = 0.8, scan = 3, memory = 7, theta = 2, gamma = 1.5, eps = 1e-08):\n",
    "    # Create our window template that we will use for convolutions\n",
    "    window = np.ones(window_width) \n",
    "    \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    \n",
    "    # Use window_width/2 as offset because convolution signal reference is \n",
    "    # at right side of window, not center of window\n",
    "    offset = int(window_width * 0.5)\n",
    "    \n",
    "    # Convolved signal of the lower vertical region\n",
    "    hist = np.sum(img[int(height * 0.8) :, :], axis=0)\n",
    "    window_signal = normalise(np.convolve(window, hist))\n",
    "    \n",
    "    # Find Left lane signal\n",
    "    [l_center, l_max] = compute_signal(window_signal, 0, int(width/2), offset, threshold)\n",
    "    l_dist = l_pred_center = 0\n",
    "    l_center_prev = l_center\n",
    "    \n",
    "    # Find Right lane signal\n",
    "    [r_center, r_max] = compute_signal(window_signal, int(width/2), len(window_signal), offset, threshold)\n",
    "    r_dist = r_pred_center = 0\n",
    "    r_center_prev = r_center\n",
    "    \n",
    "    # Initialise moving average memory and add what we found for the first layer\n",
    "    window_memory = np.empty((0,8)) # levels x [l_center, l_dist, l_max, l_disc, r_center, r_dist, r_disc, r_max]\n",
    "    window_memory = np.vstack((window_memory, [l_center, l_dist, l_center, l_max, \\\n",
    "                                               r_center, r_dist, r_center, r_max]))\n",
    "    \n",
    "    l_max_prev = l_max\n",
    "    r_max_prev = r_max\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1, (int)(height / window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        min_range = int(height - (level + 1) * window_height)\n",
    "        max_range = int(height - (level) * window_height)\n",
    "        # Compute the convolved signal from the current window\n",
    "        image_layer = np.sum(img[min_range : max_range, :], axis=0)\n",
    "        window_signal = normalise(np.convolve(window, image_layer))\n",
    "        \n",
    "        # Find the zoom range window\n",
    "        min_zoom_range = int(max(height - (level + zoom) * window_height, 0))\n",
    "        max_zoom_range = int(min(height - max((level - zoom), 0) * window_height, height))\n",
    "        # Compute the convolved signal from the current zoom level\n",
    "        region_layer = np.sum(img[min_zoom_range : max_zoom_range, :], axis=0)\n",
    "        zoom_signal = normalise(np.convolve(window, region_layer))\n",
    "        \n",
    "        if (level > 1):\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "            y_vals = np.arange(level - memory, level)[-len(window_memory[-memory:]):]\n",
    "            # find left heading\n",
    "            l_params = np.polyfit(y_vals, window_memory[-memory:,0], theta)\n",
    "            l_pred_center = np.polyval(l_params, level)\n",
    "            # find right heading\n",
    "            r_params = np.polyfit(y_vals, window_memory[-memory:,4], theta)\n",
    "            r_pred_center = np.polyval(r_params, level)\n",
    "        else:\n",
    "            l_pred_center = l_center_prev + l_dist\n",
    "            r_pred_center = r_center_prev + r_dist\n",
    "        \n",
    "        # Upper and lower bounded discount factors\n",
    "        l_pred_center = int(max(l_pred_center, 0))\n",
    "        r_pred_center = int(min(r_pred_center, width))\n",
    "        \n",
    "        ### LEFT ###\n",
    "        # Compute window and zoom signals for left\n",
    "        l_min_index = int(max(l_pred_center + offset - margin, 0))\n",
    "        l_max_index = int(min(l_pred_center + offset + margin, r_center_prev - offset))\n",
    "        # Compute the left window signal\n",
    "        [l_center, l_max] = compute_signal(window_signal, l_min_index, l_max_index, offset, threshold)\n",
    "        # Compute the left zoom region signal\n",
    "        [lz_center, lz_max] = compute_signal(zoom_signal, l_min_index, l_max_index, offset, threshold)\n",
    "        # Smooth left detectors\n",
    "        l_weights = np.array([l_max, lz_max * (1./zoom)]) + eps\n",
    "        l_center = np.average([l_center, lz_center], weights = l_weights)\n",
    "        l_max_new = l_weights.mean()\n",
    "        \n",
    "        ### RIGHT ###\n",
    "        # Compute window and zoom signals for right\n",
    "        r_min_index = int(max(r_pred_center + offset - margin, l_center_prev + offset))\n",
    "        r_max_index = int(min(r_pred_center + offset + margin, width))\n",
    "        # Compute the right window signal\n",
    "        [r_center, r_max] = compute_signal(window_signal, r_min_index, r_max_index, offset, threshold)\n",
    "        # Compute the right zoom region signal\n",
    "        [rz_center, rz_max] = compute_signal(zoom_signal, r_min_index, r_max_index, offset, threshold)\n",
    "        # Smooth right detectors\n",
    "        r_weights = np.array([r_max, rz_max * (1./zoom)]) + eps\n",
    "        r_center = np.average([r_center, rz_center], weights = r_weights)\n",
    "        r_max_new = r_weights.mean()\n",
    "        \n",
    "        # Averaged bilateral shift to avoid lane errors\n",
    "        mean_dist = np.average(window_memory[0:scan:,4] - window_memory[0:scan:,0])\n",
    "        if (l_max > r_max):\n",
    "            r_center = np.average([l_center, l_center_prev + l_dist], \\\n",
    "                                  weights = [gain, 1.0-gain]) + mean_dist\n",
    "        else:\n",
    "            l_center = np.average([r_center, r_center_prev + r_dist], \\\n",
    "                                  weights = [gain, 1.0-gain]) - mean_dist\n",
    "        \n",
    "        # Remember lane headings\n",
    "        l_dist = l_center - l_center_prev\n",
    "        r_dist = r_center - r_center_prev\n",
    "        \n",
    "        # Update max t-1\n",
    "        l_max_prev = l_max\n",
    "        r_max_prev = r_max\n",
    "        l_center_prev = l_center\n",
    "        r_center_prev = r_center\n",
    "        # Add what we found for that layer\n",
    "        window_memory = np.vstack((window_memory, [l_center, l_dist, l_pred_center, l_max_new, \\\n",
    "                                                   r_center, r_dist, r_pred_center, r_max_new]))\n",
    "    \n",
    "    return window_memory\n",
    "\n",
    "# Smoothes a path by gradient descent\n",
    "# path: a 2D array of path points\n",
    "def smooth_path(path, beta = 0.5, alpha = 0.1, tolerance = 1e-08, max_iter = 1000):\n",
    "    # Make a deep copy of path\n",
    "    output = np.copy(path)\n",
    "    # set diff to tolerance\n",
    "    diff = tolerance\n",
    "    count = 0\n",
    "    while (diff >= tolerance and count <= max_iter):\n",
    "        # reset difference\n",
    "        diff = 0\n",
    "        count += 1\n",
    "        for i in range(1, len(path)-1):\n",
    "            for j in range(len(path[i])):\n",
    "                aux = output[i][j]\n",
    "                # compute gradients\n",
    "                path_diff = (path[i][j] - output[i][j])\n",
    "                path_err = (output[i-1][j] + output[i+1][j] - 2.0 * output[i][j])\n",
    "                output[i][j] += (beta * path_diff) + (alpha * path_err)\n",
    "                # accumulate loss\n",
    "                diff += abs(aux - output[i][j])\n",
    "    # return smoothed path\n",
    "    return output\n",
    "\n",
    "### Window Drawing functions ###\n",
    "\n",
    "# Applies a window mask on the current image, centroid pair at the specified level\n",
    "# Returns: mask of the current sliding window\n",
    "def window_mask(width, height, img_ref, center, level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    if not np.isnan(center):\n",
    "        output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),\\\n",
    "               max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "# Draws the centroids returned by find_window_centroids\n",
    "# Returns: a new image with each window overlayed on the original image\n",
    "def draw_window_centroids(img, centroids, window_width, window_height, color='red'):\n",
    "    # If we found any window centers\n",
    "    if len(centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(img)\n",
    "        r_points = np.zeros_like(img)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width, window_height, img, centroids[level][0], level)\n",
    "            r_mask = window_mask(window_width, window_height, img, centroids[level][1], level)\n",
    "            # Add graphic points from window mask here to total pixels found\n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points + l_points, np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "        if (color=='red'):\n",
    "            template = np.array(cv2.merge((template, zero_channel, zero_channel)), np.uint8)\n",
    "        else:\n",
    "            template = np.array(cv2.merge((zero_channel, template, zero_channel)), np.uint8)\n",
    "        warpage = np.array(cv2.merge((img, img, img)), np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((img, img, img)),np.uint8)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Lanes and Smooth Paths\n",
    "Apply the signal processing function and then smooth the paths returned by the function.\n",
    "\n",
    "**Apply the sliding window on the perspective warped examples and show the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Sliding Window settings ###\n",
    "window_width = 60 # sliding window width\n",
    "window_height = 60 # sliding window height\n",
    "window_margin = 80 # sliding window margin\n",
    "\n",
    "window_examples = []\n",
    "for example in warped_examples:\n",
    "    centroids = find_window_centroids(example, window_width, window_height, window_margin, threshold = 0.0001, gain = 0.9)\n",
    "    centroids_smooth = smooth_path(centroids[:,[0,4]], beta=0.5, alpha = 0.5)\n",
    "    # process each centroid\n",
    "    centroid_res = draw_window_centroids(example, centroids[:,[0,4]], window_width, window_height)\n",
    "    smooth_res = draw_window_centroids(example, centroids_smooth, window_width, window_height, 'green')\n",
    "    \n",
    "    result = cv2.addWeighted(centroid_res, 1, smooth_res, 0.5, 0.0)\n",
    "    window_examples.append(result)\n",
    "# plot the results\n",
    "plot(window_examples, 2, 4, 12., 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns the x,y pairs of nonzero pixels\n",
    "def get_nonzero(img, slide = 2):\n",
    "    pairs = np.empty((0, 2))\n",
    "    for x in range(img.shape[1]):\n",
    "        if (x % slide == 0):\n",
    "            for y in range(img.shape[0]):\n",
    "                if (y % slide == 0 and img[y, x] > 0):\n",
    "                        pairs = np.vstack((pairs, [x, y]))\n",
    "    return pairs\n",
    "\n",
    "# Counts the number of lane lines visible within an image\n",
    "# WARNING: Uses high cpu resources\n",
    "# Returns: scalar\n",
    "def count_lane_lines(img, max_lanes = 4):\n",
    "    X = get_nonzero(img, slide = 4)\n",
    "    Xk = np.dot(X, X.T) # compute kernel\n",
    "    scores = []\n",
    "    for clusters in range(2, max_lanes):\n",
    "        kmeans = KMeans(n_clusters=clusters).fit(Xk)\n",
    "        label = kmeans.labels_\n",
    "        sc = silhouette_score(Xk, label, metric='euclidean')\n",
    "        scores.append(sc)\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Line Finding Pipeline ###\n",
    "class Tracker():\n",
    "    \n",
    "    # Initialises\n",
    "    # - window_width: sliding window width\n",
    "    # - window_height: sliding window height\n",
    "    # - window_margin: sliding window margin\n",
    "    # - smooth_factor: timesteps for averaging lane lines\n",
    "    # - theta: polynomial term for regression functions\n",
    "    # - threshold: value for thresholding the maximal in coordinate descent\n",
    "    # - gain: gain value to keep lanes adjacent when tracking\n",
    "    # - alpha: first smoothing value for minimising path errors\n",
    "    # - beta: second smoothing value for minimising path errors\n",
    "    def __init__(self, window_width, window_height, window_margin, smooth_factor = 3, \\\n",
    "                 theta = 2, threshold = 0.0001, gain = 0.9, alpha = 0.5, beta = 0.5, \\\n",
    "                 my_per_pix = 30 / 720, mx_per_pix = 3.7 / 700):\n",
    "        self.window_width = window_width\n",
    "        self.window_height = window_height\n",
    "        self.window_margin = window_margin\n",
    "        self.smooth_factor = smooth_factor\n",
    "        self.theta = theta\n",
    "        self.threshold = threshold\n",
    "        self.gain = gain\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.MY_per_pix = my_per_pix\n",
    "        self.MX_per_pix = mx_per_pix\n",
    "        \n",
    "        self.left_lane = None\n",
    "        self.right_lane = None\n",
    "        \n",
    "        # used for keeping track of smoothed lines\n",
    "        self.counter = 0\n",
    "    \n",
    "    # Finds and detects the adjacent left and right lane lines in the given image scene.\n",
    "    # - img: Raw input image in RGB colourspace.\n",
    "    # Returns: 2D Tuple of left and right lane line objects (see Line class).\n",
    "    def detect_lanes(self, img):\n",
    "        # mask the input image\n",
    "        mask = lane_line_mask(img, sobel_kernels, sobel_abs_thresh, sobel_mag_thresh, sobel_dir_thresh)\n",
    "        # perspective transform\n",
    "        pers_mask = camera.warp(mask)\n",
    "        # compute the window centroids\n",
    "        centroids = find_window_centroids(pers_mask, self.window_width, self.window_height, self.window_margin, \\\n",
    "                                          threshold = self.threshold, gain = self.gain, theta = self.theta)\n",
    "        # smoothed centroids\n",
    "        centroids_smooth = smooth_path(centroids[:,[0,4]], beta = self.beta, alpha = self.alpha)\n",
    "        # construct y range\n",
    "        y_vals = np.arange(0, mask.shape[0])\n",
    "        #y_vals_res = np.arange(mask.shape[0]-(self.window_height/2), 0, -self.window_height)\n",
    "        y_vals_res = np.arange(mask.shape[0], 0, -self.window_height)\n",
    "        # compute left line coefficients\n",
    "        left_coef = np.polyfit(y_vals_res, centroids_smooth[:,0], self.theta)\n",
    "        left_fit = left_coef[0] * y_vals * y_vals + left_coef[1] * y_vals + left_coef[2]\n",
    "        left_fit = np.array(left_fit, np.int32)\n",
    "        \n",
    "        # compute right line coefficients\n",
    "        right_coef = np.polyfit(y_vals_res, centroids_smooth[:,1], self.theta)\n",
    "        right_fit = right_coef[0] * y_vals * y_vals + right_coef[1] * y_vals + right_coef[2]\n",
    "        right_fit = np.array(right_fit, np.int32)\n",
    "        # window offset\n",
    "        offset_width = window_width / 2\n",
    "        \n",
    "        # construct left and right Line objects\n",
    "        left = Line(left_coef, left_fit, y_vals, offset_width)\n",
    "        right = Line(right_coef, right_fit, y_vals, offset_width)\n",
    "        \n",
    "        return (left, right)\n",
    "    \n",
    "    # Tracks and smoothes the lane lines detected from the previous detection\n",
    "    # - left: Line object\n",
    "    # - right: Line object\n",
    "    def track(self, left, right):\n",
    "        if (self.counter == 0):\n",
    "            self.left_lane = left\n",
    "            self.right_lane = right\n",
    "        else:\n",
    "            # average the lanes for n\n",
    "            self.left_lane.smooth(left, self.smooth_factor)\n",
    "            self.right_lane.smooth(right, self.smooth_factor)\n",
    "        \n",
    "        # increment counter for averaging\n",
    "        self.counter += 1\n",
    "    \n",
    "    # Resets the tracking state and clears any lane memory\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "    \n",
    "    def draw_lanes(self, img, left, right):\n",
    "        # initialise lane drawing layers\n",
    "        road_img = np.zeros_like(img)\n",
    "        road_imgbg = np.zeros_like(img)\n",
    "        \n",
    "        l_fit = left.fit()\n",
    "        r_fit = right.fit()\n",
    "        \n",
    "        inner_fit = left.combined_fit(right)\n",
    "        \n",
    "        # fill inner lane segment\n",
    "        cv2.fillPoly(road_img, [inner_fit], color = [0, 255, 0])\n",
    "        # fill left and right lane polygons\n",
    "        cv2.fillPoly(road_img, [l_fit], color = [0,0,255])\n",
    "        cv2.fillPoly(road_img, [r_fit], color = [255,0,0])\n",
    "        cv2.fillPoly(road_imgbg, [l_fit], color = [255,255,255])\n",
    "        cv2.fillPoly(road_imgbg, [r_fit], color = [255,255,255])\n",
    "        \n",
    "        return (road_img, road_imgbg)\n",
    "        \n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    # Initialises\n",
    "    #  - coefficients: 1D array of polynomial coefficients\n",
    "    #  - xvals: X points of the fitted polynomial\n",
    "    #  - yvals: Y points of the fitted polynomial\n",
    "    def __init__(self, coefficients, xvals, yvals, offset):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = True\n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xvals = [xvals]\n",
    "        # average x values of the fitted line over the last n iterations\n",
    "        self.mean_xvals = xvals\n",
    "        # recent polynomial coefficients\n",
    "        self.recent_coef = [coefficients]\n",
    "        # polynomial coefficients averaged over the last n iterations\n",
    "        self.mean_coef = coefficients\n",
    "        # difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float')\n",
    "        # x values for detected line pixels\n",
    "        self.X_vals = xvals\n",
    "        # y values for detected line pixels\n",
    "        self.Y_vals = yvals\n",
    "        # offset\n",
    "        self.offset = offset\n",
    "    \n",
    "    def fit(self):\n",
    "        return np.array(list(zip(np.concatenate((self.mean_xvals - self.offset, self.mean_xvals[::-1] + self.offset), axis = 0),\\\n",
    "                                  np.concatenate((self.Y_vals, self.Y_vals[::-1]), axis = 0))), np.int32)\n",
    "    def combined_fit(self, right):\n",
    "        return np.array(list(zip(np.concatenate((self.mean_xvals - self.offset, right.mean_xvals[::-1] + right.offset), axis = 0),\\\n",
    "                                      np.concatenate((self.Y_vals, right.Y_vals[::-1]), axis = 0))), np.int32)\n",
    "    \n",
    "    def smooth(self, line, max_count):\n",
    "        self.detected = True\n",
    "        \n",
    "        # smooth xvals over {0 < len <= n}\n",
    "        self.recent_xvals.append(line.X_vals)\n",
    "        self.recent_xvals = self.recent_xvals[-max_count:]\n",
    "        self.mean_xvals = np.sum(self.recent_xvals, axis = 0) / len(self.recent_xvals)\n",
    "        \n",
    "        # smooth coefficients over {0 < len <= n}\n",
    "        self.recent_coef.append(line.recent_coef[-1])\n",
    "        self.recent_coef = self.recent_coef[-max_count:]\n",
    "        self.mean_coef = np.sum(self.recent_coef, axis = 0) / len(self.recent_coef)\n",
    "        \n",
    "        # store additional vars\n",
    "        self.diffs = self.recent_coef[-1] - line.recent_coef[-1]\n",
    "        self.X_vals = line.X_vals\n",
    "        self.Y_vals = line.Y_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lane Curvature, Rotation and Offset\n",
    "*The following computes the respective curvatures of each lane, the rotation of the vehicle as well as its offset from the center of the lane markings.  This is important for finding where the vehicle is relative to the road and neighbouring vehicles.  Curvature of the road is also important for the adjustment of speed, i,e braking and acceleration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Curvature, Rotation and Offset ###\n",
    "\n",
    "# Take input lane line and return the curvature of the line.\n",
    "# - lane: Lane line to compute corresponding curvature\n",
    "def curvature(self, left, right):\n",
    "    # compute curvature of line (self)\n",
    "    curve_x = np.average([left.mean_xvals, right.mean_xvals], axis = 0)\n",
    "    curve_y = np.average([left.Y_vals, right.Y_vals], axis = 0)\n",
    "    curve_fit = np.polyfit(curve_y, curve_x, self.theta)\n",
    "    \n",
    "    x_val = curve_x[::-1][0]\n",
    "    line_rad = ((1. + (2. * curve_fit[0] * x_val * self.MY_per_pix + curve_fit[1]) ** 2) ** 1.5)\\\n",
    "                / np.absolute(2 * curve_fit[0])\n",
    "    return line_rad\n",
    "\n",
    "# Compute the distance from center using the current detected lines\n",
    "# - left: Left lane (Line object)\n",
    "# - right: Right lane (Line object)\n",
    "# - width: Image width\n",
    "def center_distance(self, left, right, width):\n",
    "    center_offset = (left.X_vals[-1] + right.X_vals[-1]) / 2.\n",
    "    camera_dist = (center_offset - width / 2.) * self.MX_per_pix\n",
    "    return camera_dist\n",
    "\n",
    "# patch curvature\n",
    "Tracker.curvature = curvature\n",
    "# patch center distance\n",
    "Tracker.center_distance = center_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Pipeline\n",
    "*Putting all the pieces together and showing the output from numerous video samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import Video Processors ###\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Sample and process videos ###\n",
    "\n",
    "# init tracking object with predefined params\n",
    "tracker = Tracker(window_width, window_height, window_margin)\n",
    "\n",
    "# process each video frame and return processed frame\n",
    "#  - img: Image input frame\n",
    "def process_image(img):\n",
    "    \n",
    "    result = np.copy(img)\n",
    "    result_mask = camera.warp(result)\n",
    "    \n",
    "    #signal_valid = True\n",
    "    \n",
    "    try:\n",
    "        # find lanes\n",
    "        (left, right) = tracker.detect_lanes(result)\n",
    "        # track the lanes\n",
    "        tracker.track(left, right)\n",
    "        \n",
    "    except:\n",
    "        #signal_valid = False\n",
    "        cv2.putText(result, \"Offline...\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "        \n",
    "    \n",
    "    # draw and undistort the lane images\n",
    "    (road_img, roadbg_img) = tracker.draw_lanes(result_mask, tracker.left_lane, tracker.right_lane)\n",
    "    road_unwarped = camera.unwarp(road_img)\n",
    "    roadbg_unwarped = camera.unwarp(roadbg_img)\n",
    "    \n",
    "    # overlay lane layers with original layer\n",
    "    result = cv2.addWeighted(result, 1.0, roadbg_unwarped, 0.4, 0.0)\n",
    "    result = cv2.addWeighted(result, 1.0, road_unwarped, 0.4, 0.0)\n",
    "    \n",
    "    # add stats\n",
    "    vehicle_offset = tracker.center_distance(tracker.left_lane, tracker.right_lane, img.shape[1])\n",
    "    offset_label = \"Vehicle Offset: {:.2f}m {}\".format(vehicle_offset, \"Left\" if vehicle_offset >= 0 else \"Right\")\n",
    "    curve_label = \"Lane Curvature: {:.2f}m\".format(tracker.curvature(tracker.left_lane, tracker.right_lane))\n",
    "    \n",
    "    # add statistic labels\n",
    "    stats = np.copy(result)\n",
    "    cv2.rectangle(stats, (20,img.shape[0]-150), (520, img.shape[0]-20), color=(1,1,1), thickness=-1)\n",
    "    result = cv2.addWeighted(result, 0.4, stats, 0.6, -1.0)\n",
    "    cv2.putText(result, offset_label, (50,img.shape[0]-50), cv2.FONT_HERSHEY_SIMPLEX, 1, (220,220,220), 2)\n",
    "    cv2.putText(result, curve_label, (50,img.shape[0]-100), cv2.FONT_HERSHEY_SIMPLEX, 1, (220,220,220), 2)\n",
    "    \n",
    "    # check signal validity and reset if required\n",
    "    #if (signal_valid == False):\n",
    "    #    tracker.reset()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show a sample frame of the complete pipeline prior to running sample videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_tests = [undistorted_examples[0], undistorted_examples[2]]\n",
    "final_tests_out = []\n",
    "for test in final_tests:\n",
    "    img = process_image(test)\n",
    "    final_tests_out.append(img)\n",
    "\n",
    "plot(final_tests_out, 1, 2, 12., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Process Sample:: *Project Video* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "easy_output = 'project_proc.mp4'\n",
    "clip_easy = VideoFileClip(\"project_video.mp4\")\n",
    "easy_clip = clip_easy.fl_image(process_image)\n",
    "%time easy_clip.write_videofile(easy_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(easy_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Sample:: * Challenge Video* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medium_output = 'challenge_proc.mp4'\n",
    "clip_challenge = VideoFileClip(\"challenge_video.mp4\")\n",
    "medium_clip = clip_challenge.fl_image(process_image)\n",
    "%time medium_clip.write_videofile(medium_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(medium_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Process Sample:: *Hard Challenge* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hard_output = 'harder_challenge_proc.mp4'\n",
    "clip_hard = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "hard_clip = clip_hard.fl_image(process_image)\n",
    "%time hard_clip.write_videofile(hard_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(hard_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "The pipeline initially worked quite well, with little failures in the test images.  While it does correctly identify the lanes in the project video, it fails catastrophically in the challenge videos due to extreme variations in lighting.  In the pixel detection code (see find_window_centroids) it is using region prediction as well as moving averages to make sure lanes are symmetric and to enhance detection.  This prediction method was required to avoid deviations from detecting the lane during breaks in the lane lines.\n",
    "\n",
    "To make this pipeline more robust, modifications should be made to include colour detections in the initial masking as well as using principal components analysis to normalise lane pixel regions.  This would help with failures in the window centroid finding function, which deviates due to lane region cross-over.\n",
    "\n",
    "An additional process to keep the lane detections more robust would be using a weighted moving average dependent upon variations in lane pixel positions.  This would penalise large changes in pixel positions across frames which is obvious in the hard challenge video.\n",
    "\n",
    "In conclusion, it would be interesting to see a convolutional network with recurrent connections, trained to detect lanes and compare the results..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
